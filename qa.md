# What was the criteria for the sample selection?

The sample titles were extracted from the bibliographies of the books I found in Faculty Bookshelf and that were available for immediate consultation in digital format.

# How many total works will you add to LEDoP and how do you calculate this?

The bibliography of 8 books digitally available yielded 148 public domain titles to add. Doing a simple projection (rule of three) on the remaining 11 bibliographies to screen --pertaining to the rest of books found in the Faculty Bookshelf and on each professor's page--I can calculate that the final list of LEDoP —of its first phase at least— will have a maximum of 350 titles.

# How is it possible to accomplish all that work in just one semester?

Several factors to take into consideration:

1) The programming work to manage the contents is already done. All that remains is to add the "Referenced in" and "Aggregated by" fields, as well as to adapt the styles (typography, colors, pattern) to Notre Dame's visual identity. Both are simple operations, which I will perform during the time equivalent to the preparation of a course. The rest of the work required is content aggregation, which will take up most of my semester.

2) The work of identifying titles in the public domain from bibliographies is 42 percent complete. The rest of the titles I will identify in the time equivalent to preparing a course as well.

3) I have developed technologies to facilitate and speed up the aggregation of content in the M.O.R.E.L. libraries. For example, ESAlT data is directly exported from a collection in the Zotero reference manager. This means that adding the bibliographic information for a title is sometimes just one click away.

4) Given the experience with ESAlT, I can confirm that the rate of adding works --with cover and download file--- averages 3 per hour. Given the ten hours per week that I usually take to teaching a course, this would allow me to add an average of 30 works per week. That is, in a maximum of 12 weeks I would have added all the works, leaving the remaining four weeks to test, correct, and write the user's manual.

# How do you identify which works are in the public domain?

Wikipedia mantains a [list of countries' copyright lengths](https://en.wikipedia.org/wiki/List_of_countries%27_copyright_lengths) according to the date of death and nationality of the author. When identifying a monographic work in Spanish -- or one that was originally written in Spanish but appears in translation -- in a bibliography, I check the status of the rights according to the author's biographical data, which is readily available online. If it is in the public domain, I added it to the list, in order to add it during the developing stage.

# The works are in the public domain but not the translations or critical editions cited in the bibliography, what happens in that case?

The referenced work is taken as a guide to find an edition that is in the public domain. For example, instead of:
–	Cortés, Hernán. _Letters from México_. Trans. and ed. Anthony Pagden. Orion, 1971 
I will add:
–	Cortés, Hernán. _Cartas y relaciones de Hernán Cortés al Emperador Carlos V_. Imprenta Central de los Ferro-Carriles, 1866.

# Even though a work is in the public domain, that doesn’t mean that they are already digitized. What happens when there is not a digitized version available?

From my experience with more than five hundred works added to the public domain in the M.O.R.E.L. pilot library and ESAlT, I can say that the percentage of public domain works available in digital format is very high, and is growing daily, although the ways to access them are not the most expeditious. Thanks to the search techniques and technologies that I have developed and learned, the work to identify whether a work is available or not is simple. Those works with a digital version available will be prioritized, and those that are not will be put on a waiting list for digitization at Hesburgh or through the Interlibrary Loan service.

# In which way are your libraries mobile, open, and resilient?
They are *open* because the code used is under GNU license, that is, it can be copied and improved by anyone who needs it, as long as credit is given to the original project and its openness is maintained. They are *mobile* because their design is optimized for use on small devices such as tablets and phones, and likewise the downloadable files are adapted so that their weight can be handled by small devices. And they are *resilient* because of their decentralized nature. On the one hand, the site's code is not fixed on an institutional server, but on open code hosting services such as GitHub and GitLab. On the other hand, the collection of downloadable files is hosted on Archive.org, the site of a non-profit organization that has been dedicated to preserving Internet content since 1996. These conditions allow the repository to not depend on variable circumstances, such as the availability of a particular server or administrator, but the site can be reproduced for free in multiple instances.

